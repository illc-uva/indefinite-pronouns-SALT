summary(fit)
# Make a data frame with cost (x) and complexity (y) of a 1000 points on the frontier
pareto <- data.frame(costoflanguages = seq(from = 0.89, to = 14.5, length.out = 50000))
pareto_comp <- predict(fit, newdata = pareto, se.fit = TRUE)
pareto$complexityoflanguages <- pareto_comp
pareto$type = "Pareto"
View(pareto)
synfile_exp1 = paste0(Folder, "syn_matched_exp1.csv")
allfinal_exp1 = read.csv(synfile_exp1)
p1 <- ggplot(allfinal_exp1, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#E69F00', '#293352'))+
scale_shape_manual(name="Language", values=c(16, 17))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + xlab("Communicative cost of languages") + ylab("Complexity of languages") + scale_x_continuous(limits = c(0, 15))
print(p1)
p2 <- p1 +
#geom_line(data = pareto, size = 1.5)+  geom_point(data = dominant, aes(x = costoflanguages, y = complexityoflanguages), alpha = 0.8, size = 1)
geom_line(data = pareto, size = 1.5) + ylim(0, 18)
print(p2)
synfile_exp2 = paste0(Folder, "syn_matched_exp2.csv")
allfinal_exp2 = read.csv(synfile_exp2)
# Plot
p3 <- ggplot(allfinal_exp2, aes(x=costoflanguages, y=complexityoflanguages)) +
#scale_color_manual(values=c('#E69F00', '#293352'))+
scale_color_manual(name="Language", values=c('#293352', '#E69F00'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + scale_x_continuous(limits = c(0, 15)) +  xlab("Communicative cost of languages") + ylab("Complexity of languages")
print(p3)
p4 <- p3 +
geom_line(data = pareto, size = 1.5)
#geom_line(data = pareto, size = 1.5)+  geom_point(data = dominant, aes(x = costoflanguages, y = complexityoflanguages), alpha = 0.8, size = 1)
print(p4)
natural = subset(allfinal_exp1, type == "natural")
natural = min.euc.dist(natural, pareto)
artificial = subset(allfinal_exp1, type == "artificial")
artificial = min.euc.dist(artificial, pareto)
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# PARETO FRONT
####################################
Folder = "../CSV_files/"
# Import the estimated dominant languages file
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
# Obtain the Pareto front by fitting a curve through dominant languages
fit<-nlsLM(complexityoflanguages~a/(b+costoflanguages), data = dominant, start = list(a=1,b=0))
summary(fit)
# Make a data frame with cost (x) and complexity (y) of a 50000 points on the frontier
pareto <- data.frame(costoflanguages = seq(from = 0.89, to = 14.5, length.out = 20000))
pareto_comp <- predict(fit, newdata = pareto, se.fit = TRUE)
pareto$complexityoflanguages <- pareto_comp
pareto$type = "Pareto"
####################################
# EXPERIMENT 1 PLOT
####################################
# Experiment 1 languages (matched for synonymy)
synfile_exp1 = paste0(Folder, "syn_matched_exp1.csv")
allfinal_exp1 = read.csv(synfile_exp1)
#Plot
p1 <- ggplot(allfinal_exp1, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#E69F00', '#293352'))+
scale_shape_manual(name="Language", values=c(16, 17))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + xlab("Communicative cost of languages") + ylab("Complexity of languages") + scale_x_continuous(limits = c(0, 15))
print(p1)
# Add Pareto front to the plot
png("Experiment1pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p1 +
geom_line(data = pareto, size = 1.5) + ylim(0, 18)
print(p2)
dev.off()
####################################
# EXPERIMENT 2 PLOT
####################################
# Experiment 2 languages (matched for synonymy)
synfile_exp2 = paste0(Folder, "syn_matched_exp2.csv")
allfinal_exp2 = read.csv(synfile_exp2)
# Plot
p3 <- ggplot(allfinal_exp2, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#293352', '#E69F00'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + scale_x_continuous(limits = c(0, 15)) +  xlab("Communicative cost of languages") + ylab("Complexity of languages")
print(p3)
# Add Pareto front to the plot
png("Experiment2pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p4 <- p3 +
geom_line(data = pareto, size = 1.5)
print(p4)
dev.off()
####################################
# EXPERIMENT 1 STATISTICAL ANALYSES
####################################
natural_distances = paste0(Folder, "natural_distances_pareto.csv")
artificial_distances = paste0(Folder, "artificial_distances_pareto.csv")
generate = TRUE
if(generate) {
# Natural languages distances to Pareto
natural = subset(allfinal_exp1, type == "natural")
natural = min.euc.dist(natural, pareto)
write.csv(natural, natural_distances, row.names=FALSE)
# Artificial languages distances to Pareto
artificial = subset(allfinal_exp1, type == "artificial")
artificial = min.euc.dist(artificial, pareto)
write.csv(artificial, artificial_distances, row.names=FALSE)
} else {
natural <- read.csv(natural_distances)
artificial = read.csv(artificial_distances)
}
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# PARETO FRONT
####################################
Folder = "../CSV_files/"
# Import the estimated dominant languages file
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
# Obtain the Pareto front by fitting a curve through dominant languages
fit<-nlsLM(complexityoflanguages~a/(b+costoflanguages), data = dominant, start = list(a=1,b=0))
summary(fit)
# Make a data frame with cost (x) and complexity (y) of a 50000 points on the frontier
pareto <- data.frame(costoflanguages = seq(from = 0.89, to = 14.5, length.out = 20000))
pareto_comp <- predict(fit, newdata = pareto, se.fit = TRUE)
pareto$complexityoflanguages <- pareto_comp
pareto$type = "Pareto"
####################################
# EXPERIMENT 1 PLOT
####################################
# Experiment 1 languages (matched for synonymy)
synfile_exp1 = paste0(Folder, "syn_matched_exp1.csv")
allfinal_exp1 = read.csv(synfile_exp1)
#Plot
p1 <- ggplot(allfinal_exp1, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#E69F00', '#293352'))+
scale_shape_manual(name="Language", values=c(16, 17))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + xlab("Communicative cost of languages") + ylab("Complexity of languages") + scale_x_continuous(limits = c(0, 15))
print(p1)
# Add Pareto front to the plot
png("Experiment1pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p1 +
geom_line(data = pareto, size = 1.5) + ylim(0, 18)
print(p2)
dev.off()
####################################
# EXPERIMENT 2 PLOT
####################################
# Experiment 2 languages (matched for synonymy)
synfile_exp2 = paste0(Folder, "syn_matched_exp2.csv")
allfinal_exp2 = read.csv(synfile_exp2)
# Plot
p3 <- ggplot(allfinal_exp2, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#293352', '#E69F00'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + scale_x_continuous(limits = c(0, 15)) +  xlab("Communicative cost of languages") + ylab("Complexity of languages")
print(p3)
# Add Pareto front to the plot
png("Experiment2pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p4 <- p3 +
geom_line(data = pareto, size = 1.5)
print(p4)
dev.off()
####################################
# EXPERIMENT 1 STATISTICAL ANALYSES
####################################
natural_distances = paste0(Folder, "natural_distances_pareto.csv")
artificial_distances = paste0(Folder, "artificial_distances_pareto.csv")
generate = TRUE
if(generate) {
# Natural languages distances to Pareto
natural = subset(allfinal_exp1, type == "natural")
natural = min.euc.dist(natural, pareto)
write.csv(natural, natural_distances, row.names=FALSE)
# Artificial languages distances to Pareto
artificial = subset(allfinal_exp1, type == "artificial")
artificial = min.euc.dist(artificial, pareto)
write.csv(artificial, artificial_distances, row.names=FALSE)
} else {
natural <- read.csv(natural_distances)
artificial = read.csv(artificial_distances)
}
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# PARETO FRONT
####################################
Folder = "../CSV_files/"
# Import the estimated dominant languages file
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
# Obtain the Pareto front by fitting a curve through dominant languages
fit<-nlsLM(complexityoflanguages~a/(b+costoflanguages), data = dominant, start = list(a=1,b=0))
summary(fit)
# Make a data frame with cost (x) and complexity (y) of a 50000 points on the frontier
pareto <- data.frame(costoflanguages = seq(from = 0.89, to = 14.5, length.out = 1000))
pareto_comp <- predict(fit, newdata = pareto, se.fit = TRUE)
pareto$complexityoflanguages <- pareto_comp
pareto$type = "Pareto"
####################################
# EXPERIMENT 1 PLOT
####################################
# Experiment 1 languages (matched for synonymy)
synfile_exp1 = paste0(Folder, "syn_matched_exp1.csv")
allfinal_exp1 = read.csv(synfile_exp1)
#Plot
p1 <- ggplot(allfinal_exp1, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#E69F00', '#293352'))+
scale_shape_manual(name="Language", values=c(16, 17))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + xlab("Communicative cost of languages") + ylab("Complexity of languages") + scale_x_continuous(limits = c(0, 15))
print(p1)
# Add Pareto front to the plot
png("Experiment1pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p1 +
geom_line(data = pareto, size = 1.5) + ylim(0, 18)
print(p2)
dev.off()
####################################
# EXPERIMENT 2 PLOT
####################################
# Experiment 2 languages (matched for synonymy)
synfile_exp2 = paste0(Folder, "syn_matched_exp2.csv")
allfinal_exp2 = read.csv(synfile_exp2)
# Plot
p3 <- ggplot(allfinal_exp2, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#293352', '#E69F00'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + scale_x_continuous(limits = c(0, 15)) +  xlab("Communicative cost of languages") + ylab("Complexity of languages")
print(p3)
# Add Pareto front to the plot
png("Experiment2pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p4 <- p3 +
geom_line(data = pareto, size = 1.5)
print(p4)
dev.off()
####################################
# EXPERIMENT 1 STATISTICAL ANALYSES
####################################
natural_distances = paste0(Folder, "natural_distances_pareto.csv")
artificial_distances = paste0(Folder, "artificial_distances_pareto.csv")
generate = TRUE
if(generate) {
# Natural languages distances to Pareto
natural = subset(allfinal_exp1, type == "natural")
natural = min.euc.dist(natural, pareto)
write.csv(natural, natural_distances, row.names=FALSE)
# Artificial languages distances to Pareto
artificial = subset(allfinal_exp1, type == "artificial")
artificial = min.euc.dist(artificial, pareto)
write.csv(artificial, artificial_distances, row.names=FALSE)
} else {
natural <- read.csv(natural_distances)
artificial = read.csv(artificial_distances)
}
View(pareto)
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# PARETO FRONT
####################################
Folder = "../CSV_files/"
# Import the estimated dominant languages file
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
# Obtain the Pareto front by fitting a curve through dominant languages
fit<-nlsLM(complexityoflanguages~a/(b+costoflanguages), data = dominant, start = list(a=1,b=0))
summary(fit)
# Make a data frame with cost (x) and complexity (y) of a 50000 points on the frontier
pareto <- data.frame(costoflanguages = seq(from = 0.89, to = 14.5, length.out = 5000))
pareto_comp <- predict(fit, newdata = pareto, se.fit = TRUE)
pareto$complexityoflanguages <- pareto_comp
pareto$type = "Pareto"
View(pareto)
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# PARETO FRONT
####################################
Folder = "../CSV_files/"
# Import the estimated dominant languages file
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
# Obtain the Pareto front by fitting a curve through dominant languages
fit<-nlsLM(complexityoflanguages~a/(b+costoflanguages), data = dominant, start = list(a=1,b=0))
summary(fit)
# Make a data frame with cost (x) and complexity (y) of a 50000 points on the frontier
pareto <- data.frame(costoflanguages = seq(from = 0.89, to = 14.5, length.out = 10000))
pareto_comp <- predict(fit, newdata = pareto, se.fit = TRUE)
pareto$complexityoflanguages <- pareto_comp
pareto$type = "Pareto"
View(pareto)
synfile_exp1 = paste0(Folder, "syn_matched_exp1.csv")
allfinal_exp1 = read.csv(synfile_exp1)
#Plot
p1 <- ggplot(allfinal_exp1, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#E69F00', '#293352'))+
scale_shape_manual(name="Language", values=c(16, 17))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + xlab("Communicative cost of languages") + ylab("Complexity of languages") + scale_x_continuous(limits = c(0, 15))
print(p1)
# Add Pareto front to the plot
png("Experiment1pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p1 +
geom_line(data = pareto, size = 1.5) + ylim(0, 18)
print(p2)
dev.off()
synfile_exp2 = paste0(Folder, "syn_matched_exp2.csv")
allfinal_exp2 = read.csv(synfile_exp2)
# Plot
p3 <- ggplot(allfinal_exp2, aes(x=costoflanguages, y=complexityoflanguages)) +
scale_color_manual(name="Language", values=c('#293352', '#E69F00'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.8, size = 1) + scale_x_continuous(limits = c(0, 15)) +  xlab("Communicative cost of languages") + ylab("Complexity of languages")
print(p3)
# Add Pareto front to the plot
png("Experiment2pareto-final.png", width = 140, height = 90, units='mm', res = 300)
p4 <- p3 +
geom_line(data = pareto, size = 1.5)
print(p4)
dev.off()
natural_distances = paste0(Folder, "natural_distances_pareto.csv")
artificial_distances = paste0(Folder, "artificial_distances_pareto.csv")
generate = TRUE
if(generate) {
# Natural languages distances to Pareto
natural = subset(allfinal_exp1, type == "natural")
natural = min.euc.dist(natural, pareto)
write.csv(natural, natural_distances, row.names=FALSE)
# Artificial languages distances to Pareto
artificial = subset(allfinal_exp1, type == "artificial")
artificial = min.euc.dist(artificial, pareto)
write.csv(artificial, artificial_distances, row.names=FALSE)
} else {
natural <- read.csv(natural_distances)
artificial = read.csv(artificial_distances)
}
t.test(natural$minimal, artificial$minimal)
Haspok_distances = paste0(Folder, "Haspok_distances_pareto.csv")
Haspnotok_distances = paste0(Folder, "Haspnotok_distances_pareto.csv")
if(generate) {
# Haspel ok languages distances to Pareto
Haspok <- subset(allfinal_exp2, type == "Haspel-ok")
Haspok = min.euc.dist(Haspok, pareto)
write.csv(Haspok, Haspok_distances, row.names=FALSE)
# Not Haspel ok languages distances to Pareto
Haspnotok <- subset(allfinal_exp2, type == "Not Haspel-ok")
Haspnotok = min.euc.dist(Haspnotok, pareto)
write.csv(Haspnotok, Haspnotok_distances, row.names=FALSE)
} else {
Haspok <- read.csv(Haspok_distances)
Haspnotok = read.csv(Haspnotok_distances)
}
t.test(Haspok$minimal, Haspnotok$minimal)
View(pareto)
Folder = "../data/"
data.full = read.csv(paste0(Folder, 'beekhuizen_full_set.csv'), sep = ',')
data.full.sub = droplevels(data.full[data.full$source != 'excluded' & data.full$type %in% c('body', 'thing') & data.full$annotation != 'UF',])
###################
###Translating this to flavor frequencies based on English indefinite pronouns for person
###################
data.full.sub1 = subset(data.full.sub, type == "body")
data.full.sub1$flavor = data.full.sub1$annotation
data.full.sub1$flavor <- as.character(data.full.sub1$flavor)
# If 'nobody', flavor is NQ (there is no instances of no-one in the corpus)
data.full.sub1$flavor = ifelse(data.full.sub1$en == "nobody", "NQ", data.full.sub1$flavor)
# If 'anybody/anyone' and functions question, indirect negation, direct negation, flavor is NPI
data.full.sub1$flavor = ifelse((data.full.sub1$annotation == "QU"|data.full.sub1$annotation == "IN"|data.full.sub1$annotation == "DN")&(data.full.sub1$en == "anybody"|data.full.sub1$en == "anyone"), "NPI", data.full.sub1$flavor)
# If 'anybody/anyone' and functions conditional or comparative, flavor is half of the time NPI and half of the time FCI
data.full.sub1$flavor = ifelse((data.full.sub1$annotation == "CP"|data.full.sub1$annotation == "CD")&(data.full.sub1$en == "anybody"|data.full.sub1$en == "anyone"), "NPI_FCI", data.full.sub1$flavor)
# If 'somebody/someone' and functions question, indirect negation, conditional, direct negation or comparative, flavor is NS
data.full.sub1$flavor = ifelse((data.full.sub1$annotation == "QU"|data.full.sub1$annotation == "IN"|data.full.sub1$annotation == "CD"|data.full.sub1$annotation == "DN"|data.full.sub1$annotation == "CP")&(data.full.sub1$en == "somebody"|data.full.sub1$en == "someone"), "NS", data.full.sub1$flavor)
# Otherwise, flavor == function
flavors = prop.table(xtabs(~ data.full.sub1$flavor))
#Attribute half of NPI_FCI to FC, and the other half to NPI
flavors["FCI"] = flavors["FC"] + flavors["NPI_FCI"]/2
flavors["NPI"] = flavors["NPI"] + flavors["NPI_FCI"]/2
flavors["SK"] = flavors["SP"]/2
flavors["SU"] = flavors["SP"]/2
flavors = data.frame(rbind(flavors))
flavors["NPI_FCI"] = NULL
flavors["SP"] = NULL
flavors["FC"] = NULL
names(flavors)<- tolower(names(flavors))
write.csv(flavors, paste0(Folder, 'Beekhuizen_priors.csv'), row.names=FALSE)
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# PARETO FRONT
####################################
Folder = "../data/"
# Import the estimated dominant languages file
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
####################################
# DATA
####################################
#Natural language data
####################################
# Import Haspelmath's data file: 1 = yes, 6=no, 9=no info.
# Columns DET and PERSON added by Milica based on the data from Haspelmath's appendix A with 1 meaning that the IP has respectively a DET and PERSON version, and 0 that it doesn't.
# Column neg.frag added by Milica based on data collected from literature or from competent speakers: 1 corresponds to the IP being used in constructions in which it is interpreted as a negated existential (such as negative fragment answers), 6 to it not being used. These data are made available in an online Appendix.
Folder = "../data/"
langdata = read.csv(paste0(Folder, "languages_real_40_updated.csv"), header = TRUE)
langdata[langdata == 6] <-0
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(minpack.lm)
source("./Indefinites_functions.R")
####################################
# DATA
####################################
Folder = "../data/"
# Generate a data frame with all logically possible items (in terms of which functions they can take).
allpossibilities <- expand.grid(rep(list(0:1), 10))
colnames(allpossibilities) <- c("spec.know", "spec.unkn", "irr.nonsp", "question", "condit", "indir.neg", "dir.neg", "compar", "free.ch", "neg.frag")
# Remove the one item that cannot take any of the functions.
allpossibilities = subset(allpossibilities, !(spec.know== 0 & spec.unkn ==0 & irr.nonsp ==0 & question == 0 & condit ==  0 & indir.neg ==  0 & dir.neg ==  0 & compar ==  0 & free.ch ==  0 & neg.frag == 0))
# Determine which possibilities satisfy Haspelmath's universals
allpossibilities$Haspelmath <- ifelse(((allpossibilities$spec.know == 0|allpossibilities$irr.nonsp==0)|allpossibilities$spec.unkn==1) & ((allpossibilities$spec.unkn == 0|allpossibilities$question==0)|allpossibilities$irr.nonsp==1) & ((allpossibilities$spec.unkn == 0|allpossibilities$condit==0)|allpossibilities$irr.nonsp==1) & ((allpossibilities$irr.nonsp == 0|allpossibilities$indir.neg==0)|(allpossibilities$question==1|(allpossibilities$condit==1&allpossibilities$compar==1))) & ((allpossibilities$irr.nonsp == 0|allpossibilities$compar==0)|(allpossibilities$condit==1|(allpossibilities$question==1&allpossibilities$indir.neg==1))) & ((allpossibilities$question == 0|allpossibilities$compar==0)|(allpossibilities$condit==1|allpossibilities$indir.neg==1)) & ((allpossibilities$condit == 0|allpossibilities$indir.neg==0)|(allpossibilities$question==1|allpossibilities$compar==1)) & ((allpossibilities$question == 0|allpossibilities$dir.neg==0)|allpossibilities$indir.neg==1) & ((allpossibilities$dir.neg == 0|allpossibilities$compar==0)|allpossibilities$indir.neg==1) & ((allpossibilities$condit == 0|allpossibilities$free.ch==0)|allpossibilities$compar==1) & ((allpossibilities$indir.neg == 0|allpossibilities$free.ch==0)|allpossibilities$compar==1) & (allpossibilities$spec.know==0|allpossibilities$compar==0) & (allpossibilities$spec.know==0|allpossibilities$free.ch==0) & ((allpossibilities$question+allpossibilities$condit+allpossibilities$indir.neg+allpossibilities$compar ==0)|(allpossibilities$question+allpossibilities$condit+allpossibilities$indir.neg+allpossibilities$compar > 2)) & (allpossibilities$neg.frag == 0|allpossibilities$dir.neg==1), "Haspel-ok", "Haspel-notok")
haspelpossibilities <- subset(allpossibilities, Haspelmath == "Haspel-ok")
# Whether to generate languages + measure complexity, or read the info from files
generate <- FALSE
#############################
# Generate artificial languages whose words satisfy Haspelmath's implicational map
#############################
fakefilename <- paste0(Folder, 'artificial_languages_exp2.csv')
# Extract flavors from functions
extract_flavors = function(langdata){
langdata$skflavor = ifelse(langdata$spec.know == 1,1,0)
langdata$suflavor = ifelse(langdata$spec.unkn == 1,1,0)
langdata$nsflavor = ifelse(langdata$irr.nonsp == 1,1,0)
langdata$npiflavor = ifelse(((langdata$compar == 1 & (langdata$question == 1|langdata$indir.neg == 1|langdata$free.ch == 0))|(langdata$irr.nonsp == 0 & (langdata$question == 1|langdata$indir.neg == 1))|(langdata$dir.neg == 1 & langdata$neg.frag == 0 & langdata$irr.nonsp == 0)|(langdata$condit == 1 & langdata$irr.nonsp == 0 & langdata$free.ch == 0)),1,0)
langdata$fciflavor = ifelse(langdata$free.ch == 1,1,0)
langdata$nqflavor = ifelse(langdata$neg.frag == 1,1,0)
return(langdata)
}
# Generate a number artificial languages selecting randomly items from a data frame
generate_languages = function(initial, number_lang, allitemsdf){
ITEM = c()
LANG = c()
for(i in initial:number_lang) #generate # artificial languages
{k<-sample(1:7,1) #for each language, determine how many items it will have (between 1 and 7)
for(j in 1:k){
currentitem <-paste0("fl", i,"-",j) #give a unique name to each item
currentlang <-paste0("fl", i) #give a unique name to each language
ITEM <- c(ITEM, currentitem)
LANG <-c(LANG, currentlang)
}
}
temporary = allitemsdf[FALSE,]
for(i in 1:length(ITEM)){
k<-sample(1:nrow(allitemsdf), 1)
subdf <- allitemsdf[k,]
temporary <- rbind(temporary, subdf)
}
fakelangdf <- data.frame(LANG, ITEM)
fakelangdf <- cbind(fakelangdf, temporary)
return(fakelangdf)
}
# Prep a data frame
prep = function(df){
# Determine for each item which combination of flavors it can express
df$meaning <- ifelse(df$skflavor == 1,"sk-","X-")
df$meaning <- ifelse(df$suflavor == 1,paste0(df$meaning,"su-"),paste0(df$meaning,"X-"))
df$meaning <- ifelse(df$nsflavor == 1,paste0(df$meaning,"ns-"),paste0(df$meaning,"X-"))
df$meaning <- ifelse(df$npiflavor == 1,paste0(df$meaning,"npi-"),paste0(df$meaning,"X-"))
df$meaning <- ifelse(df$fciflavor == 1,paste0(df$meaning,"fci-"),paste0(df$meaning,"X-"))
df$meaning <- ifelse(df$nqflavor == 1,paste0(df$meaning,"nq"),paste0(df$meaning,"X"))
df$meaning <- as.factor(df$meaning)
# Determine for each item how many flavors it can express
df$numoffl <- df$skflavor + df$suflavor + df$nsflavor + df$npiflavor + df$fciflavor + df$nqflavor
# Determine for each language how many items can express each of the flavors
for(flavor in c("sk", "su", "ns", "npi", "fci", "nq")){
assign(paste0("numof", flavor), setNames(aggregate(eval(parse(text = paste0(flavor, "flavor")))~LANG, data = df, sum), c("LANG", paste0("numof", flavor))))
}
df<- Reduce(function(x, y) merge(x, y, by = c("LANG"), all=TRUE), list(df, numofsk, numofsu, numofns, numofnpi, numoffci, numofnq))
return(df)
}
####################################
# COMMUNICATIVE COST AND COMPLEXITY OF A LANGUAGE: DEFINING FUNCTIONS
####################################
# Prior probability of a flavor based on Beekhuizen's corpus
priordf = read.csv("../data/Beekhuizen_priors.csv", header = TRUE)
